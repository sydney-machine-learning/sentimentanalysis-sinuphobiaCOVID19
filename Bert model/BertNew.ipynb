{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d77f63-de5e-4c52-a7a4-82865e1547e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in d:\\anaconda\\envs\\py39\\lib\\site-packages (1.1.0)\n"
     ]
    },
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "!pip install demoji\n",
    "nltk.download('all')\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff90708-ebca-48bd-80f4-2d6784bb99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78539dbb-5baa-4ea4-9900-b408ab853bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 1024\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, label_ranking_average_precision_score, f1_score\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82dd22a-5b37-4f64-9d5e-7d61470dc042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>A glass of wine keeps the corona away- DRAKE. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Can Anyone tell me if you took the flu shot la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Btw producers send me beats Im working on musi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>When someone you know.. apart of your family d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Dear soccer, I really miss you ,please come ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                              Tweet  \\\n",
       "0  1.245140e+18  A glass of wine keeps the corona away- DRAKE. ...   \n",
       "1  1.245140e+18  Can Anyone tell me if you took the flu shot la...   \n",
       "2  1.245140e+18  Btw producers send me beats Im working on musi...   \n",
       "3  1.245140e+18  When someone you know.. apart of your family d...   \n",
       "4  1.245140e+18  Dear soccer, I really miss you ,please come ba...   \n",
       "\n",
       "   Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "0           1         0           0            0        0    0        0   \n",
       "1           0         0           0            0        1    0        0   \n",
       "2           1         0           0            0        0    0        0   \n",
       "3           0         0           0            0        0    1        0   \n",
       "4           0         0           0            0        0    1        1   \n",
       "\n",
       "   Denial  Official report  Joking  \n",
       "0       0                0       1  \n",
       "1       0                0       0  \n",
       "2       0                0       1  \n",
       "3       0                0       0  \n",
       "4       0                0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senwave = pd.read_csv(\"D:\\\\Sinophobia Sentiment Analysis\\\\labeledEn.csv\")\n",
    "senwave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ee2e4d-670a-4592-a9ed-e7cc1630f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractionsWithAnotherInvertedComma = {\n",
    "\"ain’t\": \"am not\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\",\n",
    "\"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
    "\"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"he’s\": \"he is\",\n",
    "\"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
    "\"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\",\n",
    "\"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\",\n",
    "\"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
    "\"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\",\n",
    "\"shan’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\",\n",
    "\"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\",\n",
    "\"so’ve\": \"so have\", \"so’s\": \"so is\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\",\n",
    "\"there’d’ve\": \"there would have\", \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\",\n",
    "\"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\",\n",
    "\"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\",\n",
    "\"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\",\n",
    "\"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\",\n",
    "\"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
    "\"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601cb6ec-dbbe-4f06-ac1e-31865e038751",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b92ea6c-4cf4-4298-9175-19afe0181cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {\n",
    "    \"btw\": \"by the way\", \"brb\": \"be right back\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"ilu\": \"i love you\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"fb\": \"facebook\", \"ig\": \"instagram\", \"li\": \"linkedin\", \"yt\": \"youtube\", \"tw\": \"twitter\",\n",
    "    \"dm\": \"direct message\", \"mt\": \"modified tweet\", \"pm\": \"private message\", \"rt\": \"retweet\",\n",
    "    \"b2b\": \"business to business\", \"b2c\": \"business to consumer\", \"cmgr\": \"community manager\",\n",
    "    \"cms\": \"content management system\", \"cpc\": \"cost per click\", \"cpm\": \"cost per thousand impressions\",\n",
    "    \"cr\": \"conversion rate\", \"cro\": \"conversion rate optimization\", \"cta\": \"call to action\",\n",
    "    \"ctr\": \"click-through rate\", \"roi\": \"return on investment\", \"smb\": \"small and midsize businesses\",\n",
    "    \"smp\": \"social media platform\", \"smm\": \"social media marketing\", \"smo\": \"social media optimization\",\n",
    "    \"solomo\": \"social, local and mobile\", \"srp\": \"social relationship platform\", \"tos\": \"terms of service\",\n",
    "    \"ugc\": \"user-generated content\", \"api\": \"application programming interface\", \"cx\": \"customer experience\",\n",
    "    \"esp\": \"email service provider\", \"ga\": \"google analytics\", \"isp\": \"internet service provider\",\n",
    "    \"pv\": \"page views\", \"rss\": \"really simple syndication\", \"saas\": \"software as a service\",\n",
    "    \"sem\": \"search engine marketing\", \"seo\": \"search engine optimization\", \"sov\": \"share of voice\",\n",
    "    \"ui\": \"user interface\", \"url\": \"uniform resource locator\", \"uv\": \"unique views\", \"ux\": \"user experience\",\n",
    "    \"afaik\": \"as far as i know\", \"ama\": \"ask me anything\", \"btaim\": \"be that as it may\",\n",
    "    \"bts\": \"behind the scenes\", \"dae\": \"does anyone else\", \"dyk\": \"did you know\",\n",
    "    \"eli5\": \"explain like i’m five\", \"fbf\": \"flashback friday\", \"fbo\": \"facebook official\",\n",
    "    \"ff\": \"follow friday\", \"fomo\": \"fear of missing out\", \"ftfy\": \"fixed that for you\",\n",
    "    \"ftw\": \"for the win\", \"g2g\": \"got to go\", \"gg\": \"good game\", \"gtr\": \"got to run\",\n",
    "    \"hbd\": \"happy birthday\", \"hifw\": \"how i feel when\", \"hmb\": \"hit me back\", \"hmu\": \"hit me up\",\n",
    "    \"ht\": \"hat tip\", \"h/t\": \"hat tip\", \"hth\": \"here to help\", \"icymi\": \"in case you missed it\",\n",
    "    \"idc\": \"i don’t care\", \"idk\": \"i don’t know\", \"ikr\": \"i know, right?\", \"ily\": \"i love you\",\n",
    "    \"imho\": \"in my humble opinion\", \"imo\": \"in my opinion\", \"irl\": \"in real life\", \"jk\": \"just kidding\",\n",
    "    \"lmao\": \"laughing my ass off\", \"lmk\": \"let me know\", \"lms\": \"like my status\",\n",
    "    \"lol\": \"laughing out loud\", \"mcm\": \"man crush monday\", \"mfw\": \"my face when\",\n",
    "    \"mtfbwy\": \"may the force be with you\", \"nbd\": \"no big deal\", \"nm\": \"not much\",\n",
    "    \"nsfw\": \"not safe for work\", \"nvm\": \"never mind\", \"oh\": \"overheard\", \"omw\": \"on my way\",\n",
    "    \"ootd\": \"outfit of the day\", \"op\": \"original poster\", \"otp\": \"one true pairing\", \"ppl\": \"people\",\n",
    "    \"rofl\": \"rolling on the floor laughing\", \"roflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"sfw\": \"safe for work\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"tbbh\": \"to be brutally honest\",\n",
    "    \"tbt\": \"throwback thursday\", \"tfw\": \"that feeling when\", \"tgif\": \"thank god it’s friday\",\n",
    "    \"til\": \"today i learned\", \"tl;dr\": \"too long; didn’t read\", \"tmi\": \"too much information\",\n",
    "    \"wbu\": \"what about you?\", \"wbw\": \"way back wednesday\", \"wfh\": \"work from home\", \"yolo\": \"you only live once\",\n",
    "    \"afk\": \"away from keyboard\", \"asap\": \"as soon as possible\", \"atk\": \"at the keyboard\",\n",
    "    \"atm\": \"at the moment\", \"a3\": \"anytime, anywhere, anyplace\", \"bak\": \"back at keyboard\",\n",
    "    \"bbl\": \"be back later\", \"bbs\": \"be back soon\", \"bfn\": \"bye for now\", \"brt\": \"be right there\",\n",
    "    \"b4\": \"before\", \"cu\": \"see you\", \"cul8r\": \"see you later\", \"cya\": \"see you\", \"faq\": \"frequently asked questions\",\n",
    "    \"fc\": \"fingers crossed\", \"fwiw\": \"for what it's worth\", \"gal\": \"get a life\", \"gn\": \"good night\",\n",
    "    \"gmta\": \"great minds think alike\", \"gr8\": \"great!\", \"g9\": \"genius\", \"ic\": \"i see\", \"icq\": \"i seek you\",\n",
    "    \"kiss\": \"keep it simple, stupid\", \"ldr\": \"long distance relationship\", \"ltns\": \"long time no see\",\n",
    "    \"l8r\": \"later\", \"mte\": \"my thoughts exactly\", \"m8\": \"mate\", \"nrn\": \"no reply necessary\", \"oic\": \"oh i see\",\n",
    "    \"pita\": \"pain in the a..\", \"prt\": \"party\", \"prw\": \"parents are watching\", \"qpsa\": \"que pasa?\",\n",
    "    \"sk8\": \"skate\", \"stats\": \"your sex and age\", \"asl\": \"age, sex, location\", \"thx\": \"thank you\",\n",
    "    \"ttfn\": \"ta-ta for now!\", \"ttyl\": \"talk to you later\", \"u\": \"you\", \"u2\": \"you too\", \"u4e\": \"yours for ever\",\n",
    "    \"wb\": \"welcome back\", \"wtf\": \"what the f...\", \"wtg\": \"way to go!\", \"wuf\": \"where are you from?\",\n",
    "    \"w8\": \"wait...\", \"7k\": \"sick:-d laugher\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f05cbd1-9682-4572-93bd-5b0fb2378d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "class preprocess():\n",
    "    def __init__(self, df, contractions, otherContractions):\n",
    "        self.df = df\n",
    "        self.contractions = contractions\n",
    "        self.otherContractions = otherContractions\n",
    "\n",
    "    def lower(self, tweet):\n",
    "        return tweet.lower()\n",
    "\n",
    "    def abbreviate(self, tweet):\n",
    "        tweet_tokens = tweet.split(' ')\n",
    "        for i, token in enumerate(tweet_tokens):\n",
    "            cleaned_token = re.sub('[^a-zA-Z0-9-_.]', '', token)\n",
    "            if cleaned_token.lower() in abbreviations_dict:\n",
    "                tweet_tokens[i] = abbreviations_dict[cleaned_token.lower()]\n",
    "        return ' '.join(tweet_tokens)\n",
    "\n",
    "    def expand(self, tweet):\n",
    "        for word in tweet.split():\n",
    "            if word in self.contractions.keys():\n",
    "                tweet = tweet.replace(word, self.contractions[word])\n",
    "            elif word in self.otherContractions.keys():\n",
    "                tweet = tweet.replace(word, self.otherContractions[word])\n",
    "        return tweet\n",
    "\n",
    "    def emoji2text(self, tweet):\n",
    "        emojis = demoji.findall(tweet)\n",
    "        new_tweet = []\n",
    "        for word in tweet.split():\n",
    "            if word in emojis.keys():\n",
    "                tweet = tweet.replace(word, emojis[word])\n",
    "                new_tweet.append(emojis[word])\n",
    "            wordmojis = demoji.findall(word)\n",
    "            for char in word:\n",
    "                if char in wordmojis.keys():\n",
    "                    tweet = tweet.replace(word, wordmojis[char])\n",
    "\n",
    "        return tweet\n",
    "\n",
    "    def remove_hashtags(self, tweet):\n",
    "        return re.sub(r'\\#w+', '', tweet)\n",
    "\n",
    "    def remove_mentions(self, tweet):\n",
    "        for word in tweet.split():\n",
    "            if word[0] == '@':\n",
    "                tweet = tweet.replace(word, '')\n",
    "        return tweet\n",
    "\n",
    "    def remove_punctuations(self, tweet):\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')\n",
    "        return tweet.translate(trantab)\n",
    "\n",
    "    def remove_url(self, tweet):\n",
    "        return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', tweet, flags = re.MULTILINE)\n",
    "\n",
    "    def preprocess_tweet(self, tweet):\n",
    "        tweet = self.lower(tweet)\n",
    "        tweet = self.abbreviate(tweet)\n",
    "        tweet = self.expand(tweet)\n",
    "        tweet = self.emoji2text(tweet)\n",
    "        tweet = self.remove_mentions(tweet)\n",
    "        tweet = self.remove_url(tweet)\n",
    "        tweet = self.remove_hashtags(tweet)\n",
    "        tweet = self.remove_punctuations(tweet)\n",
    "\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ce1334-501c-40ad-a0b4-12b11854f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_class = preprocess(senwave, contractions, contractionsWithAnotherInvertedComma)\n",
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : pp_class.preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de93dfb8-4a53-4d1d-8a74-21e017ae9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteGen(tweet):\n",
    "    tweet = tweet.replace(\"indiavscorona\", \"india versus coronavirus\")\n",
    "    tweet = tweet.replace(\"outbreakindia\", \"outbreak india\")\n",
    "    tweet = tweet.replace(\"real”\", \"real\")\n",
    "    tweet = tweet.replace(\"mutra\", \"urine\")\n",
    "    tweet = tweet.replace(\"fakenews\", \"fake news\")\n",
    "    tweet = tweet.replace(\"“omg\", \"oh my god\")\n",
    "    tweet = tweet.replace(\"“damn\", \"damn\")\n",
    "    tweet = tweet.replace(\"god’s\", \"gods\")\n",
    "    tweet = tweet.replace(\"lockdownextension\", \"lockdown extension\")\n",
    "    tweet = tweet.replace(\"कोरोना\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"indiathanks\", \"india thanks\")\n",
    "    tweet = tweet.replace(\"coronacoronavirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace('coronavirusinsa', \"coronavirus in south africa\")\n",
    "    tweet = tweet.replace('coronaviruscanada', 'coronavirus canada')\n",
    "    tweet = tweet.replace('coronavirusau', 'coronavirus australia')\n",
    "    tweet = tweet.replace('coronavirusaus', 'coronavirus australia')\n",
    "    tweet = tweet.replace('cuomoprimetime', 'new york governor prime time')\n",
    "    tweet = tweet.replace('letsfightcoronavirus', 'let us fight coronavirus')\n",
    "    tweet = tweet.replace(\"covid19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"covid\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"aprilfoolsday\", \"april fools day\")\n",
    "    tweet = tweet.replace(\"covidー19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"stayathome\", \"stay at home\")\n",
    "    tweet = tweet.replace(\"“april\", \"april\")\n",
    "    tweet = tweet.replace(\"“i\", \"i\")\n",
    "    tweet = tweet.replace(\"aprilfools\", \"april fools\")\n",
    "    tweet = tweet.replace(\"coronavirusoutbreak\", \"coronavirus outbreak\")\n",
    "    tweet = tweet.replace(\"virusー19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fool’s\", \"fools\")\n",
    "    tweet = tweet.replace(\"what’s\", \"what is\")\n",
    "    tweet = tweet.replace(\"coronavirus”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fools”\", \"fools\")\n",
    "    tweet = tweet.replace(\"stayhome\", \"stay home\")\n",
    "    tweet = tweet.replace(\"quarantinelife\", \"quarantine life\")\n",
    "    tweet = tweet.replace(\"tablighijamaat\", \"muslims\")\n",
    "    tweet = tweet.replace(\"corona”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fauci\", \"physician\")\n",
    "    tweet = tweet.replace(\"april’s\", \"april\")\n",
    "    tweet = tweet.replace(\"pmkcallscurfewextension\", \"prime minister calls for curfew extension\")\n",
    "    tweet = tweet.replace(\"“virus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"virus”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"“corona\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronavirustruth\", \"coronavirus truth\")\n",
    "    tweet = tweet.replace(\"socialdistancing\", \"social distancing\")\n",
    "    tweet = tweet.replace(\"homestaysafe\", \"home stay safe\")\n",
    "    tweet = tweet.replace(\"“coronavirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronavirusupdate\", \"coronavirus update\")\n",
    "    tweet = tweet.replace(\"virusvirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronaviruspandemic\", \"coronavirus pandemic\")\n",
    "    tweet = tweet.replace(\"thelockdown\", \"the lockdown\")\n",
    "    tweet = tweet.replace(\"nizamuddin\", \"delhi\")\n",
    "    tweet = tweet.replace(\"trump’s\", \"donald trump\")\n",
    "    tweet = tweet.replace(\"“the\", \"the\")\n",
    "    tweet = tweet.replace(\"virus2019\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"indiafightscorona\", \"india fights coronavirus\")\n",
    "    tweet = tweet.replace(\"homesavelives\", \"home save lives\")\n",
    "    tweet = tweet.replace(\"everyone’s\", \"everyone\")\n",
    "    tweet = tweet.replace(\"coronariskforprisoners\", \"coronavirus risk for prisoners\")\n",
    "    tweet = tweet.replace(\"coronavirususa\", \"coronavirus usa\")\n",
    "    tweet = tweet.replace(\"tablighi\", \"mosque\")\n",
    "    tweet = tweet.replace(\"delhimarkaz\", \"delhi mosque\")\n",
    "    tweet = tweet.replace(\"coronajihad\", \"coronavirus struggle\")\n",
    "    tweet = tweet.replace(\"coronajihaad\", \"coronavirus struggle\")\n",
    "    tweet = tweet.replace(\"aprilfool\", \"april fool\")\n",
    "    tweet = tweet.replace(\"trumppressconference\", \"trump press conference\")\n",
    "    tweet = tweet.replace(\"i’m\", \"i am\")\n",
    "    tweet = tweet.replace(\"tigerking\", \"tiger king\")\n",
    "    tweet = tweet.replace(\"it’s\", \"it is\")\n",
    "    tweet = tweet.replace(\"trumpvirus\", \"trump virus\")\n",
    "    tweet = tweet.replace(\"today’s\", \"today is\")\n",
    "    tweet = tweet.replace(\"“you\", \"you\")\n",
    "    tweet = tweet.replace(\"“a\", \"a\")\n",
    "    tweet = tweet.replace(\"fools’\", \"fools\")\n",
    "    tweet = tweet.replace(\"rtgnews\", \"news\")\n",
    "    tweet = tweet.replace(\"19india\", \"india\")\n",
    "    tweet = tweet.replace(\"coronavirusindia\", \"coronavirus india\")\n",
    "    tweet = tweet.replace(\"y’all\", \"you all\")\n",
    "    tweet = tweet.replace(\"मीडिया\", \"media\")\n",
    "    tweet = tweet.replace(\"here’s\", \"here is\")\n",
    "    tweet = tweet.replace(\"“we\", \"we\")\n",
    "    tweet = tweet.replace(\"“fuck\", \"fuck\")\n",
    "    tweet = tweet.replace(\"flattenthecurve\", \"flatten the curve\")\n",
    "    tweet = tweet.replace(\"jammuandkashmir\", \"jammu and kashmir\")\n",
    "    tweet = tweet.replace(\"chriscuomo\", \"new york governor\")\n",
    "    tweet = tweet.replace(\"‘april\", \"april\")\n",
    "    tweet = tweet.replace(\"dranbumani\", \"doctor\")\n",
    "    tweet = tweet.replace(\"tndemandsmasstesting\", \"tamil nadu demands mass testing\")\n",
    "    tweet = tweet.replace(\"tabligi\", \"muslims\")\n",
    "    tweet = tweet.replace(\"don’t\", \"do not\")\n",
    "    tweet = tweet.replace(\"वायरस\", \"virus\")\n",
    "    tweet = tweet.replace(\"letsfightvirus\", \"let us fight virus\")\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1638781-2979-48e5-9269-10b0ae725399",
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : bruteGen(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b81394-ffb4-4288-aac5-f18c26d5f23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>can anyone tell me if you took the flu shot la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>by the way producers send me beats im working ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>when someone you know   apart of your family d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>dear soccer  i really miss you  please come ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>new home remedy to treat coronavirus  tested b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>when xavier wulf does an attack on titan tape ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>mouthwash is hand san for your mouth and i do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>yes all of them   n france 1 000 christians to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>update i destroyed the tire honestly if i get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                              Tweet  \\\n",
       "0  1.245140e+18  a glass of wine keeps the corona away  drake  ...   \n",
       "1  1.245140e+18  can anyone tell me if you took the flu shot la...   \n",
       "2  1.245140e+18  by the way producers send me beats im working ...   \n",
       "3  1.245140e+18  when someone you know   apart of your family d...   \n",
       "4  1.245140e+18  dear soccer  i really miss you  please come ba...   \n",
       "5  1.245140e+18  new home remedy to treat coronavirus  tested b...   \n",
       "6  1.245140e+18  when xavier wulf does an attack on titan tape ...   \n",
       "7  1.245140e+18  mouthwash is hand san for your mouth and i do ...   \n",
       "8  1.245140e+18  yes all of them   n france 1 000 christians to...   \n",
       "9  1.245140e+18  update i destroyed the tire honestly if i get ...   \n",
       "\n",
       "   Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "0           1         0           0            0        0    0        0   \n",
       "1           0         0           0            0        1    0        0   \n",
       "2           1         0           0            0        0    0        0   \n",
       "3           0         0           0            0        0    1        0   \n",
       "4           0         0           0            0        0    1        1   \n",
       "5           1         0           0            0        0    0        0   \n",
       "6           0         0           0            0        0    0        0   \n",
       "7           0         0           0            0        0    1        0   \n",
       "8           0         0           0            0        0    0        1   \n",
       "9           0         0           0            1        0    1        0   \n",
       "\n",
       "   Denial  Official report  Joking  \n",
       "0       0                0       1  \n",
       "1       0                0       0  \n",
       "2       0                0       1  \n",
       "3       0                0       0  \n",
       "4       0                0       0  \n",
       "5       1                0       1  \n",
       "6       0                0       1  \n",
       "7       0                0       1  \n",
       "8       0                1       0  \n",
       "9       0                0       0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senwave.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f10413c1-bded-4c8b-aa17-2ba5cbaf5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sen_train, sen_test = train_test_split(senwave, train_size = 0.9, random_state = 1024)\n",
    "\n",
    "sen_train.to_csv(\"D:\\\\Sinophobia Sentiment Analysis\\\\train.csv\", index = False)\n",
    "sen_test.to_csv(\"D:\\\\Sinophobia Sentiment Analysis\\\\test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6257c329-259b-4953-8694-5f520365235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>minister for agriculture mahendra reddy to hol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                              Tweet  \\\n",
       "560  1.245140e+18  minister for agriculture mahendra reddy to hol...   \n",
       "\n",
       "     Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "560           0         0           0            0        0    0        0   \n",
       "\n",
       "     Denial  Official report  Joking  \n",
       "560       0                1       0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658b2641-cdbe-4aaf-8d24-2a7d64604eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e19b5e-abba-425c-8466-d16200cf1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c93998-7163-49a4-a43a-c97ff5e10c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(tweet):\n",
    "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(tweet)]\n",
    "\n",
    "TWEET = Field(sequential=True, lower=True, tokenize=tokenizer)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Optimistic\", LABEL), (\"Thankful\", LABEL),\n",
    "              (\"Empathetic\", LABEL), (\"Pessimistic\", LABEL), (\"Anxious\", LABEL), (\"Sad\", LABEL),\n",
    "              (\"Annoyed\", LABEL), (\"Denial\", LABEL), (\"Official report\", LABEL), (\"Joking\", LABEL)]\n",
    "\n",
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path = \"D:\\\\Sinophobia Sentiment Analysis\\\\\", train = 'train.csv', test = 'test.csv', format = 'csv', fields = dataFields, skip_header = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71447f1b-ffea-49ad-a508-6665c943ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples : 9000\n",
      " Number of testing samples : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples : {}\\n Number of testing samples : {}\".format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222686a4-ca42-43a2-b0ce-cb9257f82896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ba91e0-c4ae-4279-bb77-37d3d4a1be99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can anyone tell me if you took the flu shot la...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>by the way producers send me beats im working ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when someone you know   apart of your family d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear soccer  i really miss you  please come ba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  a glass of wine keeps the corona away  drake  ...   \n",
       "1  can anyone tell me if you took the flu shot la...   \n",
       "2  by the way producers send me beats im working ...   \n",
       "3  when someone you know   apart of your family d...   \n",
       "4  dear soccer  i really miss you  please come ba...   \n",
       "\n",
       "                             list  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = senwave.drop(['ID'], axis = 1)\n",
    "df['list'] = df[df.columns[1:12]].values.tolist()\n",
    "new_df = df[['Tweet', 'list']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aab836c-c8e9-414f-8fe3-785c853a05bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6efed1f-f783-4684-9c0f-4a7bd55beca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b815d8e2-26fb-42ab-a7ee-d3b1652631b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200 #based on length of tweets\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-05 #tried 1e-03, 1e-04, 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c846b14-a643-435d-ade5-3f839482e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe\n",
    "        self.tweet = dataframe['Tweet']\n",
    "        self.targets = self.dataframe.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.tweet[index])\n",
    "        tweet = \" \".join(tweet.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids' : torch.tensor(ids, dtype = torch.long),\n",
    "            'mask' : torch.tensor(mask, dtype = torch.long),\n",
    "            'token_type_ids' : torch.tensor(token_type_ids, dtype = torch.long),\n",
    "            'targets' : torch.tensor(self.targets[index], dtype = torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd330490-5a1a-4c6c-acb8-b4394be390b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sen_train.drop(['ID'], axis = 1)\n",
    "train_dataset['list'] = train_dataset[train_dataset.columns[1:12]].values.tolist()\n",
    "train_df = train_dataset[['Tweet', 'list']].copy()\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "\n",
    "test_dataset = sen_test.drop(['ID'], axis = 1)\n",
    "test_dataset['list'] = test_dataset[test_dataset.columns[1:12]].values.tolist()\n",
    "test_df = test_dataset[['Tweet', 'list']].copy()\n",
    "test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d126b497-b72b-48e3-864c-88198721cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72ff4624-d3b3-4967-bb08-e136a9d7fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150bf764-83ec-4f08-b0c8-ac532cc9ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (layer1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Dropout(p=0.3, inplace=False)\n",
       "  (layer3): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.layer2 = torch.nn.Dropout(0.3)\n",
    "        self.layer3 = torch.nn.Linear(768, 10)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, return_dict = False):\n",
    "        unw, out_1 = self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[0], self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[1]\n",
    "        out_2 = self.layer2(out_1)\n",
    "        out_final = self.layer3(out_2)\n",
    "        return out_final\n",
    "\n",
    "model = BERT()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29d13608-37d4-4d8c-84c6-986f3330370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dadac344-81a7-46a8-8641-b87998c1c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for unw, data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids, return_dict = False)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if unw % 2000 == 0:\n",
    "            print(f'Iter : {unw+1}, Epoch: {epoch+1}, Loss: {total_loss/(unw+1)}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc95fa0-b27d-44db-a884-a0763fe62aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 1, Epoch: 1, Loss: 0.8155899047851562\n",
      "Iter : 2001, Epoch: 1, Loss: 0.4222854898310732\n",
      "Iter : 4001, Epoch: 1, Loss: 0.3978606775250324\n",
      "Iter : 6001, Epoch: 1, Loss: 0.3810732089724268\n",
      "Iter : 8001, Epoch: 1, Loss: 0.3692595486355385\n",
      "Iter : 1, Epoch: 2, Loss: 0.2843562364578247\n",
      "Iter : 2001, Epoch: 2, Loss: 0.2922017749404532\n",
      "Iter : 4001, Epoch: 2, Loss: 0.2904026684698329\n",
      "Iter : 6001, Epoch: 2, Loss: 0.2887180078420226\n",
      "Iter : 8001, Epoch: 2, Loss: 0.2880377938656409\n",
      "Iter : 1, Epoch: 3, Loss: 0.10525069385766983\n",
      "Iter : 2001, Epoch: 3, Loss: 0.22662378935658115\n",
      "Iter : 4001, Epoch: 3, Loss: 0.22782530802400058\n",
      "Iter : 6001, Epoch: 3, Loss: 0.22821340955163147\n",
      "Iter : 8001, Epoch: 3, Loss: 0.2284088681202064\n",
      "Iter : 1, Epoch: 4, Loss: 0.24661746621131897\n",
      "Iter : 2001, Epoch: 4, Loss: 0.16719830486325846\n",
      "Iter : 4001, Epoch: 4, Loss: 0.16895973069280928\n",
      "Iter : 6001, Epoch: 4, Loss: 0.17080156826174392\n",
      "Iter : 8001, Epoch: 4, Loss: 0.17126912329429284\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2835d179-c8aa-4471-8c37-838b91cd864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    model.eval()\n",
    "    req_targets = []\n",
    "    req_outputs = []\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for unw, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            req_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            req_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    valid_loss /= len(testing_loader)\n",
    "    return req_outputs, req_targets, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "971aace7-f3eb-4065-b69b-da78c36716e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "outputs, targets, valid_loss = valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2e565da-ccd3-4aaf-b557-feae2e3b5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(outputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52d40007-c20e-4c6c-bcd3-fe6244d9dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_outputs = np.zeros((outputs.shape[0], outputs.shape[1]))\n",
    "\n",
    "for row in range(outputs.shape[0]):\n",
    "    for col in range(outputs.shape[1]):\n",
    "        if outputs[row][col] >= 0.5: int_outputs[row][col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b44666d1-2689-4146-9344-16177b4b890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0.01991216, 0.00701787, 0.00214738, 0.36593968, 0.02075741,\n",
       "        0.08246037, 0.9865101 , 0.2256027 , 0.00240362, 0.47447339]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0], int_outputs[0], outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ae8e82d-a97e-4c6b-a285-38cafff8dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ham_loss = hamming_loss(targets, int_outputs)\n",
    "bert_jacc_score = jaccard_score(targets, int_outputs, average = 'samples')\n",
    "bert_lrap = label_ranking_average_precision_score(targets, outputs)\n",
    "bert_f1_macro = f1_score(targets, int_outputs, average = 'macro')\n",
    "bert_f1_micro = f1_score(targets, int_outputs, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01bde80c-255d-493a-b79b-fbf88c25b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.38636362420301884\n",
      "Hamming Loss: 0.1396\n",
      "Jaccard Score: 0.5137666666666667\n",
      "Label Ranking Average Precision Score: 0.7714375132275145\n",
      "F1 Macro Score: 0.5228303245854207\n",
      "F1 Micro Score: 0.5884433962264151\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss:\", valid_loss)\n",
    "print(\"Hamming Loss:\", bert_ham_loss)\n",
    "print(\"Jaccard Score:\", bert_jacc_score)\n",
    "print(\"Label Ranking Average Precision Score:\", bert_lrap)\n",
    "print(\"F1 Macro Score:\", bert_f1_macro)\n",
    "print(\"F1 Micro Score:\", bert_f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "422c1ad3-0696-4c34-8804-8fb11c4a63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f = \"D:\\\\Sinophobia Sentiment Analysis\\\\Transformers4.10Bertmodel2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b1fa7-7873-44ac-b602-86a0322e55d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
